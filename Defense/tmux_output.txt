

🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 13.102, max: 18.744



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.855, max: 16.817



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 4.810, max: 18.096



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.126, max: 18.531



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.214, max: 16.891



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 15.138, max: 17.373



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -0.961, max: 16.220



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 6.255, max: 17.234



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.321, max: 16.909



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.968, max: 18.002



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 13.347, max: 17.321



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.467, max: 17.047



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.772, max: 18.080



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.690, max: 17.952



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.259, max: 17.090



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 13.391, max: 17.352



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -0.846, max: 16.128



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.915, max: 17.496



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.748, max: 17.539



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.969, max: 16.356



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 12.828, max: 17.291



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.864, max: 15.813



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.879, max: 17.117



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.545, max: 17.970



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.406, max: 15.831



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.677, max: 17.498



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.780, max: 15.437



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 3.682, max: 17.961



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.370, max: 17.454



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.673, max: 16.004



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.517, max: 17.001



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.513, max: 15.698



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.318, max: 17.416



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.524, max: 17.354



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.783, max: 16.423



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.116, max: 17.287



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.229, max: 16.603



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 2.847, max: 17.231



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.133, max: 17.585



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.182, max: 16.661



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 15.181, max: 16.795



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.363, max: 15.985



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.672, max: 16.924



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.706, max: 17.426



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.856, max: 16.216



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.833, max: 16.638



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.241, max: 15.276



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 6.413, max: 17.041



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.536, max: 17.053



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.076, max: 18.061



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.905, max: 16.692



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.563, max: 15.556



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.201, max: 16.542



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.241, max: 17.667



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.030, max: 16.768



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.568, max: 16.832



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.648, max: 15.336



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 6.137, max: 16.695



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.921, max: 16.922



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.232, max: 17.339



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.716, max: 16.525



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.496, max: 17.706



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 6.033, max: 16.615



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.180, max: 17.371



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.499, max: 15.904



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.704, max: 16.751



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.226, max: 14.357



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 18.974, max: 21.706
 96%|█████████████████████████████████████████████████████████▌  | 1920/2000 [00:07<00:00, 248.59it/s]


 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 13.742, max: 13.852



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -0.322, max: 14.729



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.238, max: 11.012



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 13.816, max: 17.479



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 9.451, max: 10.727



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 13.440, max: 13.878



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -0.783, max: 15.226



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 1.729, max: 12.290



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 15.352, max: 18.388



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.486, max: 11.601



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 8.605, max: 14.837



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.263, max: 15.576



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 0.340, max: 13.655



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 10.786, max: 18.710



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 1.584, max: 13.785



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 8.961, max: 16.510



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.077, max: 17.277



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -0.721, max: 17.195



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 12.536, max: 19.033



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -0.154, max: 15.562



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 7.006, max: 16.676



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.537, max: 18.266



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.650, max: 16.389



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 12.668, max: 18.846



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.525, max: 15.523



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.686, max: 17.773



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.176, max: 17.888



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.718, max: 14.793



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 13.438, max: 18.761



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.363, max: 16.982



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.342, max: 17.588



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.779, max: 18.289



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.728, max: 15.620



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 12.878, max: 18.830



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.812, max: 15.312



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 4.693, max: 17.271



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.800, max: 18.301



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.267, max: 15.955



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.494, max: 17.523



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.708, max: 15.821



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 6.064, max: 18.154



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.268, max: 17.291



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.670, max: 18.036



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 15.049, max: 17.224



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.759, max: 16.435



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 4.354, max: 18.063



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.768, max: 17.844



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.061, max: 16.157



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 13.288, max: 17.365



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -0.836, max: 14.345



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 6.108, max: 17.957



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.059, max: 17.640



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.096, max: 16.634



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.281, max: 17.460



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.825, max: 16.428



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 7.284, max: 17.824



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.169, max: 17.952



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.112, max: 18.489



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.715, max: 17.373



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.280, max: 14.775



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 4.789, max: 17.739



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.759, max: 17.362



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.151, max: 16.604



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.732, max: 16.631



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.631, max: 14.773



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 4.750, max: 17.882



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.472, max: 17.538



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.617, max: 17.533



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.963, max: 17.267



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.792, max: 16.268



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 5.873, max: 18.396



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.032, max: 17.747



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.907, max: 17.329



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.678, max: 16.678



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.421, max: 16.462



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 4.049, max: 17.213



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.918, max: 17.552



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.161, max: 16.975



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.451, max: 16.725



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -4.639, max: 17.827



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 4.856, max: 17.436



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.816, max: 17.142



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.899, max: 16.340



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 13.845, max: 16.506



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.014, max: 15.274



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 4.018, max: 16.368



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.639, max: 17.182



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.165, max: 17.410



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.645, max: 16.611



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.238, max: 14.816



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 3.438, max: 16.860



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.849, max: 17.421



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -3.366, max: 17.125



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.735, max: 16.631



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.341, max: 16.361



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 3.583, max: 16.820



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -1.720, max: 17.420



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.350, max: 15.718



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 14.785, max: 16.869



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: -2.392, max: 16.189



 ############ the tendency is 7.75



🔍 Finite processed - min: 0.000, max: 1.000
🔍 Rescaled back - min: 19.213, max: 21.561
2048it [00:08, 249.24it/s]
Generated 0 invalid samples out of total 2048 samples generated. Sampling efficiency is: 100.0000%

Evaluating attack with 6000 members, 6000 non-members, 2000 synthetic records

Computing distances: 100%|█████████████████████████████████████| 12000/12000 [00:21<00:00, 547.27it/s]
[-42 -45 -44 ... -44 -43 -43]
Tendency: 7.75, AUC: 0.5485
✓ Threshold met! Computing fidelity metrics...
Computing fidelity metrics...
Error in fidelity computation: /home/infamous/miniconda3/envs/realtab/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.32' not foun
d (required by /home/infamous/.cache/keops2.3/Linux_lab1_6.8.0-58-generic_p3.10.16/nvrtc_jit.so)
Optimal tendency: 7.75, AUC: 0.5485
MMD: 0.0000
Jensen-Shannon: 0.0000
Wasserstein: 0.0000
Cleaning up model memory and disk storage...
Deleted model directory: rtf_model/id000017550020803335305216
Deleted checkpoints directory: rtf_checkpoints

==================================================
Processing num_train_length: 8000
==================================================
Training model with 8000 records...
Created member_df with 8000 records
Created non_member_df with 8000 records
Training on member data from: experiment7/member.csv
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:570: UserWarning: Duplicate rate (0
.0) in the data is zero. The `qt_interval` will be set                     to qt_interval_unique=100.
  warnings.warn(
Computing the sensitivity threshold...
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:597: UserWarning: qt_interval adjus
ted from 100 to 66...
  warnings.warn(
Using parallel computation!!!
Bootstrap round: 100%|██████████████████████████████████████████████| 500/500 [00:13<00:00, 36.57it/s]
Sensitivity threshold summary:
count    500.000000
mean       0.000286
std        0.002999
min       -0.008136
25%       -0.001982
50%        0.000212
75%        0.002358
max        0.009444
dtype: float64
Sensitivity threshold: 0.0051492194674012835 qt_max: 0.05
Map: 100%|███████████████████████████████████████████████| 8000/8000 [00:03<00:00, 2343.95 examples/s]
  0%|                                                                        | 0/1250 [00:00<?, ?it/s]ERROR with training length 8
000: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                        | 0/1250 [00:00<?, ?it/s]

Visualization saved as: experiment7/visualization_auc_experiment7.png

==================================================
Processing num_train_length: 2000
==================================================
Training model with 2000 records...
Created member_df with 2000 records
Created non_member_df with 2000 records
Training on member data from: experimentCASP/CASP.csv
Computing the sensitivity threshold...
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:597: UserWarning: qt_interval adjus
ted from 100 to 16...
  warnings.warn(
Using parallel computation!!!
Bootstrap round: 100%|████████████████████████████████████████████| 500/500 [00:00<00:00, 1058.40it/s]
Sensitivity threshold summary:
count    500.000000
mean       0.000956
std        0.006115
min       -0.015265
25%       -0.003068
50%        0.000284
75%        0.004782
max        0.023258
dtype: float64
Sensitivity threshold: 0.011249999999999998 qt_max: 0.05
Map: 100%|███████████████████████████████████████████████| 2000/2000 [00:00<00:00, 3256.54 examples/s]
  0%|                                                                         | 0/310 [00:00<?, ?it/s]ERROR with training length 2
000: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                         | 0/310 [00:00<?, ?it/s]

==================================================
Processing num_train_length: 4000
==================================================
Training model with 4000 records...
Created member_df with 4000 records
Created non_member_df with 4000 records
Training on member data from: experimentCASP/CASP.csv
Computing the sensitivity threshold...
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:597: UserWarning: qt_interval adjus
ted from 100 to 33...
  warnings.warn(
Using parallel computation!!!
Bootstrap round: 100%|█████████████████████████████████████████████| 500/500 [00:02<00:00, 212.28it/s]
Sensitivity threshold summary:
count    500.000000
mean       0.000554
std        0.004724
min       -0.011516
25%       -0.002844
50%        0.000569
75%        0.003602
max        0.014351
dtype: float64
Sensitivity threshold: 0.008896749606344796 qt_max: 0.05
Map: 100%|███████████████████████████████████████████████| 4000/4000 [00:01<00:00, 3244.20 examples/s]
  0%|                                                                         | 0/625 [00:00<?, ?it/s]ERROR with training length 4
000: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                         | 0/625 [00:00<?, ?it/s]

==================================================
Processing num_train_length: 6000
==================================================
Training model with 6000 records...
Created member_df with 6000 records
Created non_member_df with 6000 records
Training on member data from: experimentCASP/CASP.csv
Computing the sensitivity threshold...
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:597: UserWarning: qt_interval adjus
ted from 100 to 49...
  warnings.warn(
Using parallel computation!!!
Bootstrap round: 100%|██████████████████████████████████████████████| 500/500 [00:05<00:00, 95.15it/s]
Sensitivity threshold summary:
count    500.000000
mean       0.000204
std        0.003413
min       -0.009411
25%       -0.002157
50%        0.000018
75%        0.002460
max        0.011413
dtype: float64
Sensitivity threshold: 0.005949868664500768 qt_max: 0.05
Map: 100%|███████████████████████████████████████████████| 6000/6000 [00:01<00:00, 3102.98 examples/s]
  0%|                                                                         | 0/935 [00:00<?, ?it/s]ERROR with training length 6
000: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                         | 0/935 [00:00<?, ?it/s]

==================================================
Processing num_train_length: 8000
==================================================
Training model with 8000 records...
Created member_df with 8000 records
Created non_member_df with 8000 records
Training on member data from: experimentCASP/CASP.csv
Computing the sensitivity threshold...
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:597: UserWarning: qt_interval adjus
ted from 100 to 66...
  warnings.warn(
Using parallel computation!!!
Bootstrap round: 100%|██████████████████████████████████████████████| 500/500 [00:09<00:00, 52.64it/s]
Sensitivity threshold summary:
count    500.000000
mean       0.000320
std        0.003080
min       -0.008707
25%       -0.001981
50%        0.000343
75%        0.002427
max        0.010668
dtype: float64
Sensitivity threshold: 0.0054278364328896175 qt_max: 0.05
Map: 100%|███████████████████████████████████████████████| 8000/8000 [00:02<00:00, 3110.58 examples/s]
  0%|                                                                        | 0/1250 [00:00<?, ?it/s]ERROR with training length 8
000: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                        | 0/1250 [00:00<?, ?it/s]
No successful results to visualize!

==================================================
Processing num_train_length: 2000
==================================================
Training model with 2000 records...
Created member_df with 2000 records
Created non_member_df with 2000 records
Training on member data from: experimentCALIFORNIA/california.csv
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:570: UserWarning: Duplicate rate (0
.0) in the data is zero. The `qt_interval` will be set                     to qt_interval_unique=100.
  warnings.warn(
Computing the sensitivity threshold...
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:597: UserWarning: qt_interval adjus
ted from 100 to 16...
  warnings.warn(
Using parallel computation!!!
Bootstrap round: 100%|████████████████████████████████████████████| 500/500 [00:00<00:00, 1211.06it/s]
Sensitivity threshold summary:
count    500.000000
mean       0.000845
std        0.006472
min       -0.013712
25%       -0.003750
50%        0.000398
75%        0.004763
max        0.027159
dtype: float64
Sensitivity threshold: 0.013562499999999996 qt_max: 0.05
Map: 100%|███████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4475.67 examples/s]
  0%|                                                                         | 0/310 [00:00<?, ?it/s]ERROR with training length 2
000: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                         | 0/310 [00:00<?, ?it/s]

==================================================
Processing num_train_length: 4000
==================================================
Training model with 4000 records...
Created member_df with 4000 records
Created non_member_df with 4000 records
Training on member data from: experimentCALIFORNIA/california.csv
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:570: UserWarning: Duplicate rate (0
.0) in the data is zero. The `qt_interval` will be set                     to qt_interval_unique=100.
  warnings.warn(
Computing the sensitivity threshold...
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:597: UserWarning: qt_interval adjus
ted from 100 to 33...
  warnings.warn(
Using parallel computation!!!
Bootstrap round: 100%|█████████████████████████████████████████████| 500/500 [00:01<00:00, 285.77it/s]
Sensitivity threshold summary:
count    500.000000
mean       0.000315
std        0.004417
min       -0.012328
25%       -0.002828
50%        0.000106
75%        0.003349
max        0.014871
dtype: float64
Sensitivity threshold: 0.007793847566574836 qt_max: 0.05
Map: 100%|███████████████████████████████████████████████| 4000/4000 [00:00<00:00, 4183.36 examples/s]
  0%|                                                                         | 0/625 [00:00<?, ?it/s]ERROR with training length 4
000: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                         | 0/625 [00:00<?, ?it/s]

==================================================
Processing num_train_length: 6000
==================================================
Training model with 6000 records...
Created member_df with 6000 records
Created non_member_df with 6000 records
Training on member data from: experimentCALIFORNIA/california.csv
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:570: UserWarning: Duplicate rate (0
.0) in the data is zero. The `qt_interval` will be set                     to qt_interval_unique=100.
  warnings.warn(
Computing the sensitivity threshold...
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:597: UserWarning: qt_interval adjus
ted from 100 to 49...
  warnings.warn(
Using parallel computation!!!
Bootstrap round: 100%|█████████████████████████████████████████████| 500/500 [00:04<00:00, 100.35it/s]
Sensitivity threshold summary:
count    500.000000
mean       0.000342
std        0.003533
min       -0.009931
25%       -0.002114
50%        0.000246
75%        0.002716
max        0.011512
dtype: float64
Sensitivity threshold: 0.0066163677592249 qt_max: 0.05
Map: 100%|███████████████████████████████████████████████| 6000/6000 [00:01<00:00, 4129.84 examples/s]
  0%|                                                                         | 0/935 [00:00<?, ?it/s]ERROR with training length 6
000: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                         | 0/935 [00:00<?, ?it/s]

==================================================
Processing num_train_length: 8000
==================================================
Training model with 8000 records...
Created member_df with 8000 records
Created non_member_df with 8000 records
Training on member data from: experimentCALIFORNIA/california.csv
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:570: UserWarning: Duplicate rate (0
.0) in the data is zero. The `qt_interval` will be set                     to qt_interval_unique=100.
  warnings.warn(
Computing the sensitivity threshold...
/home/infamous/realtab/patt_att_visualization/REaLTabFormer/src/realtabformer/realtabformer.py:597: UserWarning: qt_interval adjus
ted from 100 to 66...
  warnings.warn(
Using parallel computation!!!
Bootstrap round: 100%|██████████████████████████████████████████████| 500/500 [00:08<00:00, 56.23it/s]
Sensitivity threshold summary:
count    500.000000
mean       0.000342
std        0.003236
min       -0.009075
25%       -0.002072
50%        0.000292
75%        0.002436
max        0.010376
dtype: float64
Sensitivity threshold: 0.005546946740128555 qt_max: 0.05
Map: 100%|███████████████████████████████████████████████| 8000/8000 [00:01<00:00, 4037.45 examples/s]
  0%|                                                                        | 0/1250 [00:00<?, ?it/s]ERROR with training length 8
000: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                        | 0/1250 [00:00<?, ?it/s]
No successful results to visualize!

============================================================
FINAL SUMMARY - EXPERIMENT7
============================================================
✓ Training length: 2000 | Optimal tendency: 12.2 | Final AUC: 0.5484 | MMD: 0.001 | JS: 0.002 | WS: 6.731
✓ Training length: 4000 | Optimal tendency:  8.5 | Final AUC: 0.5460 | MMD: 0.001 | JS: 0.001 | WS: 355.479
✓ Training length: 6000 | Optimal tendency:  7.8 | Final AUC: 0.5485 | MMD: 0.000 | JS: 0.000 | WS: 0.000

============================================================
FINAL SUMMARY - EXPERIMENTCASP
============================================================

============================================================
FINAL SUMMARY - EXPERIMENTCALIFORNIA
============================================================

All experiments completed.
(realtab) infamous@lab1:~/realtab/patt_att_visualization$ tmux capture-pane -S -; tmux save-buffer ./tmux_output.txt

